{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before run\n",
    "\n",
    "```\n",
    "# create a python virtualenv for separating the dependencies\n",
    "virtualenv virtualenv\n",
    "\n",
    "# enter the virtualenv\n",
    "source virtualenv/bin/activate\n",
    "\n",
    "# install the dependencies\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Please run the first frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_FOLDER = './data'\n",
    "\n",
    "log = print\n",
    "if not os.path.exists(DATA_FOLDER + '/evaluation'):\n",
    "    os.mkdir(DATA_FOLDER + '/evaluation')\n",
    "\n",
    "if not os.path.exists(f'{DATA_FOLDER}/Info_UserData.csv'):\n",
    "    import kaggle\n",
    "\n",
    "    kaggle.api.authenticate()\n",
    "    log('Download dataset...')\n",
    "    kaggle.api.dataset_download_files('junyiacademy/learning-activity-public-dataset-by-junyi-academy',\n",
    "                                      path=DATA_FOLDER, unzip=True)\n",
    "    log('Dataset downloaded.')\n",
    "else:\n",
    "    log(\"The dataset is present on the machine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Files in the dataset: ')\n",
    "for dirname, _, filenames in os.walk(DATA_FOLDER):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_preprocessing import extract_additional_user_features, preprocess_df, _extract_additional_user_features\n",
    "from feature_categorization import U_features\n",
    "\n",
    "\n",
    "def load_data_raw():\n",
    "    df_u = pd.read_csv(f'{DATA_FOLDER}/Info_UserData.csv')\n",
    "    df_pr = pd.read_csv(f'{DATA_FOLDER}/Log_Problem.csv')\n",
    "    df_ex = pd.read_csv(f'{DATA_FOLDER}/Info_Content.csv')\n",
    "    return df_u, df_pr, df_ex\n",
    "\n",
    "\n",
    "def remove_problems_with_total_time_outliers(df_pr):\n",
    "    limit = np.quantile(df_pr['total_sec_taken'], 0.98)\n",
    "    new_df_pr = df_pr[df_pr['total_sec_taken'] < limit]\n",
    "    return new_df_pr\n",
    "\n",
    "# if `log` is undefined then please run the first frame\n",
    "log(\"Loading data...\")\n",
    "df_u, df_pr, df_c = load_data_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr = remove_problems_with_total_time_outliers(df_pr)\n",
    "\n",
    "log(\"Feature extraction...\")\n",
    "\n",
    "\n",
    "USE_MAPREDUCE = True\n",
    "if USE_MAPREDUCE:\n",
    "    X = _extract_additional_user_features(df_u, df_pr, df_c)\n",
    "else:\n",
    "    X = extract_additional_user_features(df_u, df_pr, df_c)\n",
    "    \n",
    "print(X)\n",
    "log(\"Preprocessing...\")\n",
    "user_features = U_features()\n",
    "X: pd.DataFrame = preprocess_df(df=X, o_features=user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cure import *\n",
    "\n",
    "\n",
    "def split_users_by_grade(df):\n",
    "    split1 = df[df[\"user_grade\"] < 4]\n",
    "    split2 = df[df[\"user_grade\"] == 5]\n",
    "    split3 = df[df[\"user_grade\"] == 6]\n",
    "    split4 = df[df[\"user_grade\"] == 7]\n",
    "    split5 = df[df[\"user_grade\"] > 7]\n",
    "    return [split1, split2, split3, split4, split5], [\"split1\", \"split2\", \"split3\", \"split4\", \"split5\"]\n",
    "\n",
    "\n",
    "def cure_clustering(df: pd.DataFrame):\n",
    "    data = df.drop(columns=[\"uuid\"]).to_numpy()\n",
    "    cure_repres = cure_representatives(data)\n",
    "    cluster_labels = cure_classify(data, cure_repres)\n",
    "    n_clusters = cluster_labels.max() + 1\n",
    "    partition = lambda k: data[cluster_labels == k]\n",
    "    similarities = [pairwise_distances(partition(k)) for k in range(n_clusters)]\n",
    "    sim_users = [df[\"uuid\"][cluster_labels == k] for k in range(n_clusters)]\n",
    "    return cluster_labels, similarities, sim_users\n",
    "\n",
    "\n",
    "USE_USER_USER_SIMILARITY = True\n",
    "all_split_labels = []\n",
    "all_split_similarities = []\n",
    "all_split_sim_users = []\n",
    "\n",
    "log(\"Splitting users...\")\n",
    "dfs, labels = split_users_by_grade(X)\n",
    "del X\n",
    "\n",
    "log('Cluster each instance of the split...')\n",
    "for df, label in zip(dfs, labels):\n",
    "    log(\"Getting clusters for {} users...\".format(label))\n",
    "    cluster_labels, similarities, sim_users = cure_clustering(df)\n",
    "    all_split_labels.append(cluster_labels)\n",
    "    all_split_similarities.append(similarities)\n",
    "    all_split_sim_users.append(sim_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "\n",
    "def evaluate_clusterings(combined):\n",
    "    for df, split, labels in combined:\n",
    "        df = df.drop(columns=[\"uuid\", \"first_login_date_TW\"])\n",
    "        data = df.to_numpy()\n",
    "        log(f\"Split: {split} | Davies-Bouldin score: \", davies_bouldin_score(data, labels))\n",
    "        centroids = [np.average(data[labels == k], axis=0) for k in range(labels.max() + 1)]\n",
    "\n",
    "        def interesting_columns(x: np.ndarray):\n",
    "            return list(filter(lambda pair: pair[1] > 0.07, zip(df.columns, x)))\n",
    "\n",
    "        c0 = centroids[0]\n",
    "        log(f\"Centroid differences (for {len(centroids)} clusters):\")\n",
    "        for i in range(1, min(3, len(centroids))):\n",
    "            c1 = centroids[i]\n",
    "            cdiff = interesting_columns(c0 - c1)[:3]\n",
    "            log(cdiff)\n",
    "        log('---')\n",
    "\n",
    "log('Evaluate clusters in each split')\n",
    "log('---')\n",
    "evaluate_clusterings(zip(dfs, labels, all_split_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from recommender_system import *\n",
    "\n",
    "\n",
    "def bind_labels_and_uuid(cluster_labels, sim_users):\n",
    "    cluster_partitions = [cluster_labels[cluster_labels == i] for i in range(cluster_labels.max() + 1)]\n",
    "    clusters = pd.DataFrame([np.hstack(cluster_partitions), np.hstack(sim_users)]).T\n",
    "    clusters.columns = [\"labels\", \"uuid\"]\n",
    "    return clusters\n",
    "\n",
    "\n",
    "log(\"Binding clusters labels and uuids for all splits\")\n",
    "all_segment_clusters = [bind_labels_and_uuid(c_labels, s_users) for (c_labels, s_users) in\n",
    "                        zip(all_split_labels, all_split_sim_users)]\n",
    "\n",
    "\n",
    "def run_and_evaluate_recommender_system(clusters, df_pr, df_u, user_user_similarities, cluster_id=0,\n",
    "                                        use_user_user_similarity=False):\n",
    "    M, M_test, U1_ids, P1_ids = generate_utility_matrix_for_one_cluster(clusters=clusters, df_u_full=df_u,\n",
    "                                                                        df_pr_full=df_pr, cluster_id=cluster_id)\n",
    "\n",
    "    cluster_user_user_similarity = user_user_similarities[cluster_id]\n",
    "\n",
    "    difficulties_for_all_users, errors_all = get_psedu_problem_difficulties(M, M_test, cluster_user_user_similarity,\n",
    "                                                                            use_user_user_similarity)\n",
    "    errors = [item[0] for sublist in errors_all for item in sublist if len(item) > 0]\n",
    "    mean_abs_error = np.mean(errors)\n",
    "    recommendation_difficulty_for_all_users, recommendation_idx_all = get_recommendation(difficulties_for_all_users)\n",
    "    num_users = clusters.loc[clusters['labels'] == cluster_id].shape[0]\n",
    "    print(\"Mean absolute error of difficulty was {} for cluster {} with {} users and use_similarrity={}\".format(\n",
    "        mean_abs_error, cluster_id, num_users, str(use_user_user_similarity)))\n",
    "    return mean_abs_error, errors, recommendation_difficulty_for_all_users, recommendation_idx_all, np.mean(\n",
    "        difficulties_for_all_users[difficulties_for_all_users > 0])\n",
    "\n",
    "\n",
    "# Run all splits, and all clusters\n",
    "mean_errors = []\n",
    "for split_idx, clusters_ in enumerate(all_segment_clusters):\n",
    "    df_u_split = dfs[split_idx]\n",
    "    df_p_split = df_pr.loc[df_pr['uuid'].isin(df_u_split['uuid'])]\n",
    "    similarities_ = all_split_similarities[split_idx]\n",
    "    for cluster_idx in tqdm(range(len(similarities_)), desc=\"running cluster\"):\n",
    "        mean_abs_error, errors, recommendation_difficulty_for_all_users, recommendation_idx_all, mean_difficulty = run_and_evaluate_recommender_system(\n",
    "            clusters_, df_p_split, df_u_split, similarities_, cluster_idx, USE_USER_USER_SIMILARITY)\n",
    "        mean_errors.append(mean_abs_error)\n",
    "        with open(f'{DATA_FOLDER}/evaluation/eval_mean_errors_5splits.txt', 'a') as f:\n",
    "            f.write(\n",
    "                \"split_id: {},split_size: {}, cluster_id: {},cluster_u_size {}, n_errors: {}, mean_error: {}, mean_difficulty: {}, mean_recommendation_difficulty: {}\\n\".format(\n",
    "                    split_idx, df_u_split.shape[0], cluster_idx, similarities_[cluster_idx].shape[0],\n",
    "                    len(errors), np.round(mean_abs_error, 5), np.round(mean_difficulty, 5),\n",
    "                    np.round(np.mean(recommendation_difficulty_for_all_users), 5)))\n",
    "        with open(f'{DATA_FOLDER}/evaluation/eval_error_5splitss.txt', 'a') as f:\n",
    "            f.write(\"split_id: {}, cluster_id: {}, errors {}\\n\".format(split_idx, cluster_idx, errors))\n",
    "print(\"Mean absolute errors for the different splits {}\".format(mean_errors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}